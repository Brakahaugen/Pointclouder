[03/19 14:40:31 d2.evaluation.coco_evaluation]: 'balloon_test' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...
WARNING [03/19 14:40:32 d2.data.datasets.coco]: Using previously cached COCO format annotations at './utput/balloon_test_coco_format.json'. You need to clear the cache file if your dataset has been modified.
[03/19 14:40:32 d2.data.build]: Distribution of instances among all 1 categories:
|  category  | #instances   |
|:----------:|:-------------|
|    tree    | 60           |
|            |              |
[03/19 14:40:32 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[03/19 14:40:32 d2.data.common]: Serializing 1 elements to byte tensors and concatenating them all ...
[03/19 14:40:32 d2.data.common]: Serialized dataset takes 0.01 MiB
[03/19 14:40:32 d2.evaluation.evaluator]: Start inference on 1 images
[03/19 14:40:32 d2.evaluation.evaluator]: Inference done 1/1. 0.1168 s / img. ETA=0:00:00
[03/19 14:40:32 d2.evaluation.evaluator]: Total inference time: 0:00:00.226850 (0.226850 s / img per device, on 1 devices)
[03/19 14:40:32 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:00 (0.116818 s / img per device, on 1 devices)
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Saving results to ./utput/coco_instances_results.json
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
COCOeval_opt.evaluate() finished in 0.00 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.00 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.506
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.186
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.098
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|
| 20.534 | 50.590 | 18.638 | 21.286 |  nan  |  nan  |
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
COCOeval_opt.evaluate() finished in 0.00 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.00 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.460
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.107
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|
| 21.564 | 46.007 | 20.767 | 22.121 |  nan  |  nan  |
[03/19 14:40:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.
OrderedDict([('bbox', {'AP': 20.53447530002231, 'AP50': 50.589855937483954, 'AP75': 18.63801186492552, 'APs': 21.2855628264686, 'APm': nan, 'APl': nan}), ('segm', {'AP': 21.563851808217706, 'AP50': 46.00707259316029, 'AP75': 20.76729569903254, 'APs': 22.12127244470797, 'APm': nan, 'APl': nan})])